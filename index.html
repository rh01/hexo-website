<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hello, IT's" />





  <link rel="alternate" href="/atom.xml" title="恒行天下" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.0" />






<meta name="description" content="我是一名 AI 爱好者，着习于机器学习和深度学习。">
<meta property="og:type" content="website">
<meta property="og:title" content="恒行天下">
<meta property="og:url" content="http://blog.shenhengheng.xyz/index.html">
<meta property="og:site_name" content="恒行天下">
<meta property="og:description" content="我是一名 AI 爱好者，着习于机器学习和深度学习。">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="恒行天下">
<meta name="twitter:description" content="我是一名 AI 爱好者，着习于机器学习和深度学习。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"right"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://blog.shenhengheng.xyz/"/>





  <title> 恒行天下 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  




<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-92073513-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?14b57a75b075b6fd8618498ca810584c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-right 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">恒行天下</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle">生恒爱之 生恒敬之</p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>


 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://blog.shenhengheng.xyz/2017/12/24/2017-12-23-svm/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Shen Hengheng">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="http://olrs8j04a.bkt.clouddn.com/17-12-24/90775656.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="恒行天下">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="恒行天下" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/24/2017-12-23-svm/" itemprop="url">
                  支持向量机算法
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-24T08:00:00+08:00">
                Dec 24 2017
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ml/" itemprop="url" rel="index">
                    <span itemprop="name">ml</span>
                  </a>
	
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ml/svm/" itemprop="url" rel="index">
                    <span itemprop="name">svm</span>
                  </a>
	
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ml/svm/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
	
                </span>

                
                
              
            </span>
			
			
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/12/24/2017-12-23-svm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/12/24/2017-12-23-svm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
		  
	
		  
		  
		  

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://blog.shenhengheng.xyz/2017/12/23/2017-12-23-hmm/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Shen Hengheng">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="http://olrs8j04a.bkt.clouddn.com/17-12-24/90775656.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="恒行天下">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="恒行天下" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/23/2017-12-23-hmm/" itemprop="url">
                  隐马尔可夫模型
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-23T08:00:00+08:00">
                Dec 23 2017
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog/" itemprop="url" rel="index">
                    <span itemprop="name">blog</span>
                  </a>
	
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog/ml/" itemprop="url" rel="index">
                    <span itemprop="name">ml</span>
                  </a>
	
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog/ml/hmm/" itemprop="url" rel="index">
                    <span itemprop="name">hmm</span>
                  </a>
	
                </span>

                
                
              
            </span>
			
			
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/12/23/2017-12-23-hmm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/12/23/2017-12-23-hmm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
		  
	
		  
		  
		  

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>\noindent 很早就想写一篇关于<strong>隐马尔可夫模型</strong>的文章了，这次刻意的将模型及其有关算法复习了一下，才有了这个信心去写了这篇文章。这篇文章主要参考了李航老师的《统计机器学习^[<a href="https://www.amazon.cn/dp/B007TSFMTA/ref=sr_1_1?ie=UTF8&amp;qid=1514031854&amp;sr=8-1&amp;keywords=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95]》" target="_blank" rel="external">https://www.amazon.cn/dp/B007TSFMTA/ref=sr_1_1?ie=UTF8&amp;qid=1514031854&amp;sr=8-1&amp;keywords=%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95]》</a>  部分内容和徐亦达老师在油管上的教程^[<a href="https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLyAft-JyjIYoc9LN241WKqLPuggfSBBpt]。并且这里的隐马尔可夫模型主要是指**离散形式的动态模型**。" target="_blank" rel="external">https://www.youtube.com/watch?v=Ji6KbkyNmk8&amp;list=PLyAft-JyjIYoc9LN241WKqLPuggfSBBpt]。并且这里的隐马尔可夫模型主要是指**离散形式的动态模型**。</a></p>
<p>隐马尔可夫模型主要是一种<strong>可用于标注问题的统计学习模型</strong>，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型。近些年来主要用于语音信号处理，自然语言处理，生物信息，金融分析等领域，该教程涉及很多概率计算问题，所以希望读者能够有概率背景的情况下，阅读更佳。</p>
<p>本部分主要介绍一下几个部分：</p>
<ul>
<li>HMM现象</li>
<li>马尔可夫过程/链</li>
<li>算法<ul>
<li>直接计算法</li>
<li>前向-后向算法<ul>
<li>前向算法</li>
<li>后向算法</li>
</ul>
</li>
</ul>
</li>
<li>学习算法</li>
<li>总结</li>
</ul>
<h1 id="HMM现象"><a href="#HMM现象" class="headerlink" title="HMM现象"></a>HMM现象</h1><p>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.6\textwidth]{hmm4.png}<br>  \caption{HMM现象}\label{figure:hmm}<br>\end{center}<br>\end{figure}</p>
<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/13276902.jpg" alt=""></p>
<p>玩股票的朋友都知道，图\ref{figure:hmm}右，在市场规则下，股民只知道股票的涨停和不变这三种表面现象，而人们并不希望仅仅知道市场的涨停，而是想知道市场隐藏的信息，比如现在股票市场是熊市还是牛市，因为知道这些隐藏的信息之后，我们可以利用这些信息去长期跟进还是及时退出，以防市场大变。在这里我们称这种人们表面上能观察出来的现象为观测（observation），而那些隐藏的变量被称作状态。</p>
<p>有了上面比较直观的介绍以后，我们就可以定义什么是隐马尔可夫模型了。</p>
<p><strong>定义10.1（隐马尔可夫模型）</strong>　隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列（state sequence）；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列（observation sequence）。序列的每一个位置又可以看作是一个时刻。</p>
<p>比如：上图的Bull-&gt;Bear-&gt;Event就是其中一个状态序列，而其产生的观测形成的序列up-&gt;down-&gt;unchange被称作观测序列，而这些序列在一定假设下，具有非常好的概率性质。</p>
<h1 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h1><p>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.6\textwidth]{markov.png}<br>  \caption{HMM的概率图模型}\label{figure:markov}<br>\end{center}<br>\end{figure}<br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/10240319.jpg" alt=""></p>
<p>上图\ref{figure:markov}是隐马尔可夫的概率图模型，概率图反应了事件的依赖和独立关系。</p>
<p>首先我们要说明q表示状态，不可观测。y表示可观测的现象。假设状态与状态之间、状态与观测现象之间均满足马尔可夫过程，其中该过程要求具备“无记忆”的性质（马尔可夫性质）：下一状态的概率分布只能由当前状态决定，在时间序列中前面的事件均与之无关。 用数学语言表示为：</p>
<p>$$\Pr(X<em>NaN=x\mid X</em>{1}=x<em>{1},X</em>{2}=x<em>{2},\ldots ,X</em>{n}=x<em>{n})=\Pr(X</em>NaN=x\mid X<em>{n}=x</em>{n})$$</p>
<p>那么根据概率图模型，我们得出了两个重要的公式：</p>
<p><strong>离散</strong>状态转移概率：<br>$$p(q_t |q<em>1, . . . , q</em>{t−1}, y<em>1, . . . , y</em>{t−1}) = p(q<em>t |q</em>{t−1})$$</p>
<p><strong>离散/连续</strong>观测概率：<br>$$p(y_t |q<em>1, . . . , q</em>{t−1}, y<em>1, . . . , y</em>{t−1})  = p(y_t |q_t )$$</p>
<p>那么有了这个公式我们可以计算出图\ref{figure:markov}的转移概率矩阵，计算过程见图\ref{figure:hmm2}<br>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.6\textwidth]{hmm2.png}<br>  \caption{状态转移概率计算}\label{figure:hmm2}<br>\end{center}<br>\end{figure}</p>
<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/98038385.jpg" alt=""></p>
<p>$A$是状态转移概率矩阵,$A$描述了状态之间转换关系及其分布。</p>
<p>其中$a<em>{ij}=P(i</em>{t+1}=q<em>j|i</em>{t}=q_i),\; \; i=1,2,…,N;j=1,2,…,N;$， 是在时刻$t$处于状态$q_i$的条件下在时刻$t+1$转移到状态$q_j$ 的概率。</p>
<p>同样地，我们可以计算出观测概率矩阵，具体计算过程见\ref{figure:hmm3}<br>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.6\textwidth]{hmm3.png}<br>  \caption{观测概率的计算}\label{figure:hmm3}<br>\end{center}<br>\end{figure}</p>
<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/34045629.jpg" alt=""></p>
<p>$B$是观测概率矩阵, 其中$b<em>j(k)=P(o</em>{t}=v<em>k|i</em>{t}=q_j)\; \; k=1,2,…,M;j=1,2,…,N;$， 是在时刻$t$处于状态$q_j$的条件下生成观测$v_k$的概率。</p>
<p>有了这些性质之后我们就可以计算观测序列的概率了。比如利用图\ref{figure:hmm2}和图\ref{figure:hmm3}计算$Pr(y_1=up, y_2 = up, y_3=down)$的概率.</p>
<p>\begin{align<em>}<br>P(y_1,y_2,y<em>3) &amp;= \sum</em>{q<em>1=1}^{k} \sum</em>{q<em>2=1}^{k} \sum</em>{q_3=1}^{k} P(y_1,y_2,y_3,q_1,q_2,q<em>3) \<br>&amp;= \sum</em>{q<em>1=1}^{k} \sum</em>{q<em>2=1}^{k} \sum</em>{q_3=1}^{k} P(y_3|y_1,y_2,q_1,q_2,q_3)P(y_1,y_2,q_1,q_2,q<em>3) \<br>&amp;= \sum</em>{q<em>1=1}^{k} \sum</em>{q<em>2=1}^{k} \sum</em>{q_3=1}^{k} P(y_3|q_3)P(y_1,y_2,q_1,q_2,q<em>3) \<br>&amp;= \sum</em>{q<em>1=1}^{k} \sum</em>{q<em>2=1}^{k} \sum</em>{q_3=1}^{k} P(y_3|q_3)P(q_3|y_1,y_2,q_1,q_2)P(y_1,y_2,q_1,q<em>2) \<br>&amp;= \sum</em>{q<em>1=1}^{k} \sum</em>{q<em>2=1}^{k} \sum</em>{q_3=1}^{k} P(y_3|q_3)P(q_3|q_2)P(y_1,y_2,q_1,q<em>2) \<br>&amp;= \sum</em>{q<em>1=1}^{k} \sum</em>{q<em>2=1}^{k} \sum</em>{q_3=1}^{k} P(y_3|q_3)P(q_3|q_2)P(y_2|y_1,q_1,q_2)P(y_1,q_1,q<em>2) \<br>&amp;= \sum</em>{q<em>1=1}^{k} \sum</em>{q<em>2=1}^{k} \sum</em>{q_3=1}^{k} P(y_3|q_3)P(q_3|q_2)P(y_2|q_2)P(y_1,q_1,q<em>2) \<br>&amp;= \sum</em>{q<em>1=1}^{k} \sum</em>{q<em>2=1}^{k} \sum</em>{q_3=1}^{k} P(y_3|q_3)P(q_3|q_2)P(y_2|q_2)P(q_2|q_1)P(y_1|q_1)P(q_1)<br>\end{align</em>}</p>
<p>但是发现除了A和B我们已知，还无法计算$P(y_1,y_2,y_3)$，还需要知道$P(q_1)$的分布情况，所以在这里引出了另外一个条件，$P(q_1)$的分布。也就是初始状态概率。为了符号化，这里$\pi$是初始状态概率向量$\pi = (\pi_i)$： 其中$\pi_i = P(i_1 = q_i)$， 是时刻t＝1处于状态$q_i$ 的概率。</p>
<p><strong>总结：</strong><br>隐马尔可夫模型由初始状态概率向量$\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。$\pi$和$A$决定状态序列，$B$决定观测序列。因此，隐马尔可夫模型$\lambda$可以用三元符号表示，即<br>$$\lambda=(A,B,\pi)$$<br>$A$,$B$,$\pi$称为隐马尔可夫模型的三要素。</p>
<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>HMM的三个基本计算问题：<br>\begin{align<em>}<br> &amp;\text{Evaluate} \; \; p(Y|\lambda) \<br> &amp;\lambda<em>{MLE} = \text{argmax}</em>{\lambda}p(Y|\lambda) \<br> &amp;\text{argmax}_Q p(Y|Q,\lambda)<br>\end{align</em>}</p>
<h2 id="直接计算法"><a href="#直接计算法" class="headerlink" title="直接计算法"></a>直接计算法</h2><p>首先解决第一个问题，$\text{Evaluate} \; \; p(Y|\lambda)$,</p>
<p>\begin{align<em>}<br>P(Y|\lambda) &amp;= \sum<em>{Q}[p(Y,Q|\lambda)] =\sum</em>{q<em>1=1}^{k} …, \sum</em>{q_T=1}^{k} [p(y_1,…,y_T,q_1,…,q<em>T|\lambda)] \<br>&amp;=\sum</em>{q<em>1=1}^{k} …, \sum</em>{q_T=1}^{k} [p(y_1,…,y_T,q_1,…,q<em>T|\lambda)] \<br>&amp;= \sum</em>{q<em>1=1}^{k} …,\sum</em>{q_3=1}^{k} p(q_1)p(y_1|q_1)p(q_2|q_1)…p(q<em>t|q</em>{t-1}p(y_t|q<em>t))\<br>&amp;= \sum</em>{q<em>1=1}^{k} …,\sum</em>{q_3=1}^{k} \pi(p<em>1)\amalg</em>{t=2}^{T}a<em>{q</em>{t-1},q<em>t}b</em>{q_t}(y_t)<br>\end{align</em>}</p>
<p>其中:<br>\begin{align<em>}<br>p(q<em>t = j|q</em>{t−1} = i) &amp;\equiv a_{i,j} \<br>p(y_t |q_t = j) &amp;\equiv bj (y_t)<br>\end{align</em>}<br>但是，利用上面公式计算量很大，是$O(TN^T)$阶的，这种算法不可行。</p>
<h2 id="前向-后向算法"><a href="#前向-后向算法" class="headerlink" title="前向-后向算法"></a>前向-后向算法</h2><h3 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h3><p><strong>定义10.2（前向概率）</strong>　给定隐马尔可夫模型，定义到时刻t部分观测序列为$o_1,o_2,…,o_t$且状态为$q_i$的概率为前向概率，记作<br>$$\alpha_t(i)=P(o_1,o_2,…,o_t,i_t=q_t|\lambda)$$</p>
<p>下面是前向算法的概率图模型。</p>
<p>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.6\textwidth]{forward.png}<br>  \caption{前向算法的概率图模型}\label{figure:forward}<br>\end{center}<br>\end{figure}</p>
<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/35121312.jpg" alt=""></p>
<p>前向过程：<br>$$\alpha_t(i)=P(o_1,o_2,…,o_t,i_t=q<em>t|\lambda) \Longrightarrow p(Y|\lambda) = \sum</em>{i=1}^{k} \alpha_i(T)$$</p>
<p>递推:对$t＝1,2,…，T-1$，</p>
<p>\begin{align<em>}<br>\alpha_i(1) &amp;= p(y_1,q_1=i|\lambda) = p(q_1)p(y_1|q_1)=\pi_i b_i(y_1) \<br>\alpha_j(2) &amp;= p(y_1,y_2,q<em>2=j|\lambda) = \sum</em>{i=1}^{k}p(q_1=i)p(y1|q_1=i)p(q_2 = i | q_1 = i)p(y_2 | q<em>2 = j) \<br>&amp;= [\sum</em>{i=1}^k \alpha<em>i(1)\alpha</em>{i,j}]b_j(y_2) = p(q_1)p(y_1|q_1)=\pi_ib_i(y_1) \<br>&amp;… \<br>\alpha<em>j(t+1) &amp;=[\sum</em>{i=1}^k \alpha<em>i(t)\alpha</em>{i,j}]b<em>j(y</em>{t+1}) \<br>&amp;… \<br>\alpha<em>j(T) &amp;=[\sum</em>{i=1}^k \alpha<em>i(T-1)\alpha</em>{i,j}]b<em>j(y</em>{T})<br>\end{align</em>}</p>
<h3 id="后向算法"><a href="#后向算法" class="headerlink" title="后向算法"></a>后向算法</h3><p><strong>定义10.3（后向概率）</strong>　给定隐马尔可夫模型，定义在时刻t状态为$q<em>i$的条件下，从t+1到T的部分观测序列为$o</em>{t+1}$,$o_{t+2}$,…,$o_T$的概率为后向概率，记作<br>$$\beta<em>t(i)=P(o</em>{t+1},o<em>{t+2},o</em>{t+3},…,o_T|i_t=q_i,\lambda)$$</p>
<p>可以用递推的方法求得后向概率$\beta_t(i)$及观测序列概率$P(O|\lambda)$。<br>$$\beta<em>i (t) = p(y</em>{t+1}, . . . , y_T |q_t = i, \lambda)$$<br>下面是后向算法的概率图模型。</p>
<p>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.6\textwidth]{backward.png}<br>  \caption{后向算法的概率图模型}\label{figure:backward}<br>\end{center}<br>\end{figure}<br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-23/98688168.jpg" alt=""></p>
<p>后向过程：<br>$$\beta<em>i (t) = p(y</em>{t+1}, . . . , y_T |q<em>t = i, \lambda) \Longrightarrow \sum</em>{i=1}^k \beta_i(1)\pi_ib_i(y_1)=p(Y|\lambda)$$</p>
<p>迭代过程：</p>
<p>\begin{align<em>}<br>\beta_i(T) &amp;= 1 \<br>\beta_i(T-1) &amp;= p(y<em>T|q</em>{T-1} = i) = \sum_{j=1}^kp(q<em>T=j|q</em>{T-1}=i)p(y_T|q<em>T=j)=\sum</em>{j=1}^{k}a_{i,j}b_j(T) \<br>\beta_i(T-2) &amp;= p(y<em>T,y</em>{T-1}|q<em>{T-2} = i) \<br>&amp;= \sum</em>{j=1}^k \sum_{l=1}^k p(q<em>T=l|q</em>{T-1}=j)p(y_T|q<em>T=l) p(q</em>{T-1}=j|q<em>{T-2}=i)p(y</em>{T-1}|q<em>{T-1}=j)\<br>&amp;=\sum</em>{j=1}^{k}a_{i,j}b<em>j(y</em>{T-1})\beta_j(T-1) \<br>&amp;…\<br>\beta<em>i(t) &amp;=\sum</em>{j=1}^k\alpha_{i,j}b_j(y_t+1)\beta_j(t + 1) \<br>&amp;…\<br>\beta<em>i(1)&amp;=\sum</em>{j=1}^k\alpha_{i,j}b_j(y_2)\beta_j(2)<br>\end{align</em>}</p>
<p>在时刻t处位于序列$Y$状态$q_i$时的概率：</p>
<p>$$p(q_t = i|Y,\lambda) = \frac{p(Y, q_t = i|\lambda)}{p(Y|\lambda)}=\frac{p(Y, q<em>t = i|\lambda)}{\sum</em>{j=1}^{k}p(Y, q_t =j|\lambda)}= \frac{\alpha_i(t)\beta<em>i{t}}{\sum</em>{j=1}^{k}\alpha_j(t)\beta_j(t)}$$</p>
<p>\begin{align<em>}<br>p(Y, q_t = i|\lambda) &amp;= p(Y|q_t = i)p(q_t = i)\<br>&amp;= p(y_1, . . . y_t |q<em>t = i)p(y</em>{t+1},… y_T |q_t = i)p(q_t = i) \<br>&amp;= p(y_1, . . . y_t , q<em>t = i)p(y</em>{t+1},… y_T |q_t = i) \<br>&amp;= \alpha_i(t)\beta_i(t)<br>\end{align</em>}</p>
<h1 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h1><p>后期更新，主要讲解学习算法之EM算法，利用EM算法来学习未知参数。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://blog.shenhengheng.xyz/2017/12/21/2017-12-21-introduction-machine-learning/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Shen Hengheng">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="http://olrs8j04a.bkt.clouddn.com/17-12-24/90775656.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="恒行天下">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="恒行天下" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/21/2017-12-21-introduction-machine-learning/" itemprop="url">
                  机器学习简介
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-21T08:00:00+08:00">
                Dec 21 2017
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pi/" itemprop="url" rel="index">
                    <span itemprop="name">pi</span>
                  </a>
	
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/pi/ml/" itemprop="url" rel="index">
                    <span itemprop="name">ml</span>
                  </a>
	
                </span>

                
                
              
            </span>
			
			
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/12/21/2017-12-21-introduction-machine-learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/12/21/2017-12-21-introduction-machine-learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
		  
	
		  
		  
		  

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>\noindent 该笔记是来自 Andrew Ng 的 Machine Learning 课程的第一周的课堂记录，主要讲解了以下几个内容:</p>
<ul>
<li>机器学习的定义</li>
<li>机器学习的分类<ul>
<li>第一类：监督学习</li>
<li>第二类：学习理论</li>
<li>第三类：非监督学习</li>
<li>第四类：强化学习</li>
</ul>
</li>
</ul>
<h1 id="什么是机器学习？"><a href="#什么是机器学习？" class="headerlink" title="什么是机器学习？"></a>什么是机器学习？</h1><h2 id="非正式定义"><a href="#非正式定义" class="headerlink" title="非正式定义"></a>非正式定义</h2><p>1959年， Arthur Samuel^[发明了西洋棋程序]非正式地定义了机器学习：“在不直接针对问题进行编程的情况下，赋予计算机学习能力的一个研究领域”。</p>
<p>西洋棋就是一个例子，西洋棋自己和自己下棋。由于计算机程序的处理速度非常快，所以Arthur Samuel让计算机与计算机自己下了成千上万盘棋，逐渐地，西洋棋意识到了怎样的局势能使自己胜利，什么样的局势导致自己失败，它会反复地自我学习：“如果让对手占据这些地方时，那么我输的概率可能比较大”，或者“如果我占据这些地方时，我胜利的概率比较大”。在1959年，奇迹出现，他的西洋棋程序的棋艺远远超过西洋棋程序的作者！</p>
<p>过去人们的看法是计算机除了做程序明确让其做的事情，除外它什么都不能做，但是Arthur Samuel做到了！</p>
<h2 id="现代化的定义"><a href="#现代化的定义" class="headerlink" title="现代化的定义"></a>现代化的定义</h2><p>Tom Mitchell在1998年提出现代化的机器学习的定义：“一个合理的学习问题应该是这样定义的：对于一个计算机程序来说，给它一个任务$T$和一个性能评测方法$P$，如果在经验$E$的影响下，$P$对$T$的测量结果得到了改进，那么就说明程序从中学习到了经验$E$.</p>
<p>比如对于西洋棋那个例子来说：</p>
<ul>
<li>$E$ - 程序成千上万次的自我练习</li>
<li>$T$ - 下棋</li>
<li>$P$ - 它与人类棋手对奕的概率</li>
</ul>
<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><p>下面是给予一组房屋大小与对应的房屋价格的数据进行拟合的例子： </p>
<p>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.8\textwidth]{price_prediction.png}<br>  \caption{房屋价格预测模型}\label{figure:price_prediction}<br>\end{center}<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/2994829.jpg" alt=""><br></center>

<p>通过学习来预测房屋价格的问题是监督学习问题的一个例子，之所以成为监督学习，是因为我们这个算法提供了一组房屋的大小($size$)和一组某种程度上可以堪称正确答案的房屋价格$Price$的数据。比如(1000, 30)等。</p>
<p>监督问题的学习，给算法提供了一组”正确“的输入和”标准“答案，之后，我们希望算法能够去学习标准输入和标准答案之间的联系，以尝试对于我们提供的其他输入来给我们提供<strong>更为</strong>标准的答案。</p>
<h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>分类问题是监督学习问题中的另一类问题，它与回归问题最大的区别在于，此时的数据是离散的我而不是连续的.</p>
<p>下面是给予肿瘤的大小和对应的肿瘤是否是恶性（1）或者良性（0），来进行拟合学习最佳的分类决策线。其中</p>
<ul>
<li>数据是一组关于乳腺癌的数据，$X$代表肿瘤的大小，$Y$代表肿瘤的是否为恶性</li>
<li>目标是让一个算法学会预测一个肿瘤是否是恶性或良性</li>
</ul>
<p>很显然，这是一个<strong>2分类</strong>问题。具体如图2所示：<br>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.8\textwidth]{tumor_classifict.png}<br>  \caption{肿瘤恶性/良性分类模型}\label{tumor_classifict}<br>\end{center}<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/41356628.jpg" alt=""><br></center>


<p>下面考虑一组多个输入变量（多个特征）的数据，下面以两个特征进行举例说明。其中$X$表示肿瘤的大小，$Y$表示患者的年龄，对于图中的样本数据的表示进行说明：’o’表示良性，’x’表示为恶性，目标是找出最佳的分类决策边界。如图3所示：<br>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.5\textwidth]{tumor_classifict_2.png}<br>  \caption{多特征的肿瘤恶性/良性分类模型}\label{tumor_classifict_2}<br>\end{center}<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/66479699.jpg" alt=""><br></center>

<p>如果你的数据不能在二维，三维甚至任何有限维空间表示出来，如果你觉得数据实际上存在于无线维空间中该怎么办？</p>
<p><strong>solution:</strong>支持向量机算法^[kernel method]，可以将数据映射到无限维空间，所以他不仅能处理像之前例子中的两个特征所表示的数据，而且还可以处理无限种特征.</p>
<h1 id="学习理论"><a href="#学习理论" class="headerlink" title="学习理论"></a>学习理论</h1><p>学习理论主要主要试图了解一下什么算法能够很好地近似不同的函数，并且试图了解一些诸如需要多少训练集数据，测试集数据这样的问题，还比如算法优化，欠拟合和过拟合等问题。</p>
<h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><p>\begin{figure}[htbp]<br>\centering                                                          %居中<br>\subfigure[一些样本数据]{                    %第一张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.3]{unsupervised_1.png}               %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\subfigure[聚类之后]{                    %第二张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.3]{unsupervised_2.png}                %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\caption{你能在这组数据中寻找一些有趣的结构吗？} %                         %大图名称<br>\label{figure:unsupervised}                                                        %图片引用标记<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/85085279.jpg" alt=""><br></center>

<p>一个算法会寻找这样的结构（如图\ref{figure:unsupervised}）,它会将里面的数据聚成两类，”聚类问题“就是无监督学习的一个典型问题。主要应用：</p>
<ul>
<li>无监督学习尝试理解基因数据，会按照基因在实验中体现出的形状的规律，来对单独的基因数据进行分类。</li>
<li>google news尝试对新闻进行聚类。</li>
<li>聚类算法处理图像问题</li>
</ul>
<p>特定的无监督学习算法，它会学习对这些像素进行聚类，就是说，这些像素可能是在一起的，那些像素可能是在一起的。对像素进行的分组，他们对于计算机视觉和图像处理领域都很有用。</p>
<p>\begin{figure}[htbp]<br>\centering                                                          %居中<br>\subfigure[对单独的基因进行分类]{                    %第一张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.3]{gene.png}               %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\subfigure[google news对新闻数据进行聚类]{                    %第二张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.25]{google_news.png}                %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\caption{应用} %                         %大图名称<br>\label{figure:cluster}                                                        %图片引用标记<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/60672521.jpg" alt=""><br></center>

<p>还有一些例子，比如计算机集群，社会网络分析，市场划分，航天数据分析等等都会用到无监督学习。另外还有在语音识别方面的经典问题<a href="https://baike.baidu.com/item/%E9%B8%A1%E5%B0%BE%E9%85%92%E4%BC%9A%E9%97%AE%E9%A2%98" target="_blank" rel="external">鸡尾酒会问题</a>.</p>
<h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p>他可以用在一些你不需要进行一次决策的情形中，比如，利用监督学习对癌症进行预测的例子中，对于一个病人，你要预测他的肿瘤是否为恶性，那么你的预测将会决定病人的生死，也就是说，你的决策要么对要么错。那么对于强化学习来说，他主要使用了一种叫做<strong>回报函数</strong>的概念来进行决策，不像监督学习那样一次性决策，它是探索性算法它常常用于机器人领域，网页爬取等领域。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://blog.shenhengheng.xyz/2017/12/21/2017-12-21-single-variable-linear-regression/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Shen Hengheng">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="http://olrs8j04a.bkt.clouddn.com/17-12-24/90775656.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="恒行天下">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="恒行天下" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/21/2017-12-21-single-variable-linear-regression/" itemprop="url">
                  单变量线性回归
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-21T08:00:00+08:00">
                Dec 21 2017
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog/" itemprop="url" rel="index">
                    <span itemprop="name">blog</span>
                  </a>
	
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog/comment/" itemprop="url" rel="index">
                    <span itemprop="name">comment</span>
                  </a>
	
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/blog/comment/ml/" itemprop="url" rel="index">
                    <span itemprop="name">ml</span>
                  </a>
	
                </span>

                
                
              
            </span>
			
			
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/12/21/2017-12-21-single-variable-linear-regression/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/12/21/2017-12-21-single-variable-linear-regression/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
		  
	
		  
		  
		  

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>\noindent 该笔记是来自 Andrew Ng 的 Machine Learning 课程的第二周:<strong>单变量线性回归</strong>的课堂记录，主要讲解了以下几个内容:</p>
<ul>
<li>代价函数</li>
<li>梯度下降算法</li>
</ul>
<h1 id="模型表达"><a href="#模型表达" class="headerlink" title="模型表达"></a>模型表达</h1><p>还是上一个关于房屋价格预测的例子，给你一组房屋的大小和对应房屋的价格的数据，让你对数据进行建模，使得对于其他的房屋的大小更好的拟合出更准确的价格出来。如图1所示。<br>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.5\textwidth]{single_ex_1.png}<br>  \caption{房屋价格预测模型}\label{figure:single_ex_1}<br>\end{center}<br>\end{figure}</p>
<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/50192377.jpg" alt=""></p>
<p class="caption"><br>图 1: 房屋价格预测模型<br></p>

<p>题外话：首先数据是一组关于标准输入与标准答案的数据集，那么对该数据进行建模，是属于监督学习任务。又因为数据只含有一个变量（$size$）即一个特征，且找到一条直线来拟合数据集，所以该问题又被称为单变量线性回归问题。</p>
<p>图2是部分的样本数据，其中$X$表示房屋的大小，$Y$表示对应房屋的价格。<br>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.5\textwidth]{s_data.png}<br>  \caption{数据}\label{figure:s_data}<br>\end{center}<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/25995934.jpg" alt=""><br></center><br><p class="caption"><br>图 2: 数据<br></p><br><br>为了后面更好的描述，所以对一些符号进行声明：<br><br>- $m$ 训练集中实例的数量<br>- $x’s$ 输入变量或者特征<br>- $y’s$ 输出变量或目标变量<br>- $(x,y)$ 一个训练样本<br>- $(x^{(i)},y^{(i)})$ 第$i$个训练样本<br>\end{itemize}<br><br>有了数据之后，可以用下图就可以描述房屋价格预测问题建模的过程。<br>\begin{figure}[htbp]<br>\begin{center}<br>  \includegraphics[width=0.5\textwidth]{procedure.png}<br>  \caption{建模的过程}\label{figure:precedure}<br>\end{center}<br>\end{figure}<br><center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/49786727.jpg" alt=""><br></center><br><p class="caption"><br>图 3: 建模的过程<br></p><br><br>图片描述：因而解决房屋价格预测问题，实际上就是要将训练集”喂给“我们的学习算法，进而学习到一个假设$h$，然后将要预测的房屋尺寸作为输入变量输入给$h$，预测出该房屋的交易价格作为输出结果。<strong>其实$h$就是一个我们要学习的函数</strong>。<br><br>我们如何表示函数呢？对于单变量线性回归问题来说，$h<em>{\theta}(x) = \theta</em>{0} + \theta<em>{1} x$.<br><br># 代价函数<br>## 引入代价函数<br>有了训练数据，也有了模型的表示$h$，但是问题是如何选择 $\theta</em>{0}$ 和 $\theta<em>{1}$ ?<br><br>基本想法是选择$\theta</em>{0}$，$\theta<em>{1}$以使得在训练集上$h</em>{\theta}(x)$接近$y$.用数学表达式表示出来，如下：<br>$$minimize<em>{\theta{0},\theta</em>{1}}\frac{1}{2m}\sum<em>{i=1}^{m}(h</em>{\theta}(x^{(i)})- y^{(i)})^{2}$$<br><br><strong>注意：</strong> 在学习的时候，有一个区别，$loss function$描述的是单个样本误差，而$cost$ $function$描述整个训练集的误差。<br><br>同样地，为了更好的下文描述，将其简化：<br><br>$$J(\theta<em>{0},\theta</em>{1}) = \frac{1}{2m}\sum<em>{i=1}^{m}(h</em>{\theta}(x^{(i)})- y^{(i)})^{2}$$<br><br>Goals: <code>$$ minimize_{\theta{0},\theta{1}}J(\theta_{0}, \theta_{1})$$</code><br>其中$J(\theta<em>{0}, \theta</em>{1})$是平方误差（代价）函数，它是解决回归问题常用的手段。<br><br>## 代价函数$J$的工作原理<br><br><br>- 假说函数:    $h<em>{\theta}(x) = \theta</em>{0} + \theta<em>{1}x$<br>- 参数：$\theta</em>{0}, \theta<em>{1}$<br>- 代价函数：$J(\theta</em>{0},\theta<em>{1}) = frac{1}{2m}\sum</em>{i=1}^{m}(h<em>{\theta}(x^{(i)})- y^{(i)})^{2}$<br>- 目标：$minimize</em>{\theta{0},\theta{1}}J(\theta<em>{0}, \theta</em>{1})$<br><br>简化来讲，就是找到使得误差最小的那一对参数（$\theta<em>{0}, \theta</em>{1}$).<br><br>\begin{figure}[htbp]<br>    \begin{center}<br>        \includegraphics[width=0.8\textwidth]{cost.png}<br>        \caption{代价函数工作原理}\label{figure:cost}<br>    \end{center}<br>\end{figure}<br><br><p class="caption"><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/26673069.jpg" alt=""><br>图 4: 代价函数工作原理<br></p><br><br><br>## 代价函数的直观理解<br>引入一种表示方法：轮廓图。我们在三维空间来表示（$\theta<em>{0}, \theta</em>{1}, J(\theta<em>{0}, \theta</em>{1})$)，如图\ref{figure:contour}<br><br>\begin{figure}[htbp]<br>    \begin{center}<br>        \includegraphics[width=0.5\textwidth]{contour.png}<br>        \caption{轮廓图}\label{figure:contour}<br>    \end{center}<br>\end{figure}<br><center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/12948998.jpg" alt=""><br></center><br><p class="caption"><br><br>图 5: 轮廓图<br></p><br><br><br><br>可以直观地得到以下结论：在曲面的最低的那个点对应的代价误差最小，对应的（$\theta<em>{0}, \theta</em>{1}$)是我们要找的参数组合。<br><br>还有一种表示方法：等高线图，他的工作原理是，每一圈上的值都是相等的，即对应的代价误差是相同的。从外到里，代价不断减小，最里面的是最小的代价误差，也是我们算法要最终学习到的点。如图\ref{figure:contour_2}<br><br><br>\begin{figure}[htbp]<br>    \begin{center}<br>        \includegraphics[width=0.5\textwidth]{contour_2.png}<br>        \caption{等高线图}\label{figure:contour<em>2}<br>    \end{center}<br>\end{figure}<br><center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/66180792.jpg" alt=""><br></center><br><p class="caption"><br>图 6: 等高线图<br></p><br><br># 梯度下降算法<br>## 算法描述<br>我们很迫切想求得最佳拟合参数（$\theta</em>{0}, \theta<em>{1}$），但是有不想通过枚举的方式求得，因此需要一个算法能够自动求出使得代价函数$J(\theta</em>{0}, \theta<em>{1})$取得最小值的参数$\theta</em>{0}, \theta_{1}$.<br><br>所以在这里引入梯度下降算法。主要思想是想象你在一个山丘上，怎么样以最快的速度从山上到山脚下？在这里就引入了梯度的概念，即下降速度最快的方向，所以人没走一步就检查一下，当前是否为下降最快的方向？若不是则将重新求梯度，按照梯度指示的方向走，则为最快的方式！将这种思想应用到求使得代价误差函数最小，也是这么做的。但是这样往往带来几个问题？<br><br>1. 以多大的脚步往下走，因为如果脚步过大，则会造成错过最佳的下山路线。（学习速率选择问题）<br>2. 由于不一定你的目标函数是凸函数，所以每次走可能走到不同”的山脚“。（局部最小值，凸优化问题）如图\ref{figure:gradient}<br><br><br>这些问题将会在后面一一解决！<br><br>\begin{figure}[htbp]<br>\centering                                                          %居中<br>\subfigure[梯度下降]{                    %第一张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.3]{gradient_intu.png}               %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\subfigure[走到局部最低点]{                    %第二张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.3]{gradient_intu<em>2.png}                %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\caption{梯度下降算法} %                         %大图名称<br>\label{figure:gradient}                                                        %图片引用标记<br>\end{figure}<br><center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/65682294.jpg" alt=""><br></center><br>下面是梯度下降算法的为伪代码。<br><br><br><br><br>\begin{algorithm}[h]<br>  \caption{梯度下降算法简化版}<br>  \label{alg::Gradient Descent Simple}<br>  \begin{algorithmic}[1]<br>    \Require<br>    $\alpha$ 学习率;<br>    $\theta</em>{0} \in R, \theta<em>{1}\in R$ 参数;<br>    $J$ 代价误差函数;<br>    $:=$ 赋值;<br>    \Repeat<br>      \State $\theta</em>{j} := \theta<em>{j} - \alpha\frac{\partial}{\partial</em>{j}}J(\theta<em>{0}, \theta</em>{1})$ for $j = 0 $和$j = 1 $;<br>      \State (向量版本)$\theta :=  \theta - \alpha\nabla J(\theta<em>{0}, \theta</em>{1})$;<br>    \Until{收敛}<br>  \end{algorithmic}<br>\end{algorithm}<br><center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/96169106.jpg" alt=""><br></center>

<p>下图\ref{figure:gradient_change_theta} 直观的描述$\theta$是如何变化的.这里$\alpha &gt; 0$.</p>
<p>\begin{figure}[htbp]<br>\centering                                                          %居中<br>\subfigure[$\frac{\partial}{\partial<em>{\theta</em>{1}}}J(\theta<em>{0}, \theta</em>{1}) &gt; 0$时，$\theta_{1}$往右移动]{                    %第一张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.5]{gradient_change<em>theta.png}               %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\subfigure[$\frac{\partial}{\partial</em>{\theta<em>{1}}}J(\theta</em>{0}, \theta<em>{1}) &lt; 0$时，$\theta</em>{1}$往左移动]{                    %第二张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.5]{gradient_change_theta_1.png}                %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\caption{代价误差不断接近最低处} %                         %大图名称<br>\label{figure:gradient_change_theta}                                                        %图片引用标记<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/29605590.jpg" alt=""><br></center>

<h2 id="学习率-alpha"><a href="#学习率-alpha" class="headerlink" title="学习率$\alpha$"></a>学习率$\alpha$</h2><p>学习率在算法迭代时起着很大的作用。<br>\begin{enumerate}<br>\setlength{\itemsep}{1pt}<br>\setlength{\parsep}{0pt}<br>\setlength{\parskip}{0pt}</p>
<ul>
<li>如果$\alpha$过小，将会导致算法的收敛速度很慢，算法的效率低;</li>
<li>如果$\alpha$过大，在梯度下降过程中，很容易掠过最小值，可能会无法收敛甚至发散。如图\ref{figure:alpha}.<br>\end{enumerate}</li>
</ul>
<p>\begin{figure}[htbp]<br>\centering                                                          %居中<br>\subfigure[$\alpha$过小，导致收敛速度很慢]{                    %第一张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.5]{alpha_low.png}               %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\subfigure[$\alpha$过大，导致无法到达最低点]{                    %第二张子图<br>\begin{minipage}{7cm}<br>\centering                                                          %子图居中<br>\includegraphics[scale=0.5]{alpha_high.png}                %以pic.jpg的0.5倍大小输出<br>\end{minipage}}<br>\caption{不同的$\alpha$对算法收敛的影响} %                         %大图名称<br>\label{figure:alpha}                                                        %图片引用标记<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/75838488.jpg" alt=""><br></center>

<p>虽然选择的是合适的$alpha$，但是算法还是会掉到局部最小值，对于局部最小值的描述如图\ref{figure:local_minimize}</p>
<p>\begin{figure}[htbp]<br>    \begin{center}<br>        \includegraphics[width=0.6\textwidth]{local_minimize.png}<br>        \caption{局部最小值}\label{figure:local_minimize}<br>    \end{center}<br>\end{figure}</p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/23570380.jpg" alt=""><br></center>

<p>在算法迭代时，随着逐渐接近最小值，步子会慢慢地变小，直到接近最小值。所以在算法的迭代过程中不需要对$\alpha$改变。</p>
<p>\subsubsection{算法详细版}</p>
<ul>
<li>假说函数:    $h<em>{\theta}(x) = \theta</em>{0} + \theta_{1}x$</li>
<li>参数：$\theta<em>{0}, \theta</em>{1}$</li>
<li>代价函数：$J(\theta<em>{0},\theta</em>{1}) = \frac{1}{2m}\sum<em>{i=1}^{m}(h</em>{\theta}(x^{(i)})- y^{(i)})^{2}$</li>
<li>目标：$minimize<em>{\theta{0},\theta{1}}J(\theta</em>{0}, \theta_{1})$</li>
<li>求解：$\theta<em>{j} := \theta</em>{j} - \alpha \frac{\partial}{\partial<em>{\theta</em>{j}}} J(\theta<em>{0}, \theta</em>{1})$ for $j=0$ 和 $j=1$</li>
</ul>
<p>结合上面的公式可以求得<br>$$ \theta<em>{0} := \theta</em>{0} - \alpha \frac{1}{m} \sum<em>{i=1}^{m}(h</em>{\theta}(x^{(i)}) - y^{(i)})$$<br>$$ \theta<em>{1} := \theta</em>{1} - \alpha \frac{1}{m} \sum<em>{i=1}^{m}(h</em>{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$</p>
<p>算法的伪代码如下：</p>
<p>\begin{algorithm}[h]<br>  \caption{梯度下降算法简化版}<br>  \label{alg::Gradient Descent Simple}<br>  \begin{algorithmic}[1]<br>    \Require<br>    $\alpha$ 学习率;<br>    $\theta<em>{0} \in R, \theta</em>{1}\in R$ 参数;<br>    $:=$ 赋值;<br>    \Repeat<br>      \State $\theta<em>{0} := \theta</em>{0} - \alpha \frac{1}{m} \sum<em>{i=1}^{m}(h</em>{\theta}(x^{(i)}) - y^{(i)})$;<br>      \State $\theta<em>{1} := \theta</em>{1} - \alpha \frac{1}{m} \sum<em>{i=1}^{m}(h</em>{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$;<br>    \Until{收敛}<br>  \end{algorithmic}<br>\end{algorithm}  </p>
<center><br><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/55414484.jpg" alt=""><br></center>

<ul>
<li>由于代价误差函数是二次函数，二次项为负所以图像为“弓（拱）”形，所以是<strong>凸函数</strong>，所以算法总能收敛到全局最小值.</li>
<li>由于训练的过程中，将全部的训练数据喂给我们的模型，所以该过程又称<strong>“批量训练过程”</strong>.</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="http://blog.shenhengheng.xyz/2017/12/21/2017-12-21-overfitting/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Shen Hengheng">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="http://olrs8j04a.bkt.clouddn.com/17-12-24/90775656.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="恒行天下">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="恒行天下" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/12/21/2017-12-21-overfitting/" itemprop="url">
                  过拟合和正则化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-12-21T08:00:00+08:00">
                Dec 21 2017
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ml/" itemprop="url" rel="index">
                    <span itemprop="name">ml</span>
                  </a>
	
                </span>

                
                
              
            </span>
			
			
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/12/21/2017-12-21-overfitting/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/12/21/2017-12-21-overfitting/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
		  
	
		  
		  
		  

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>\noindent 该笔记是来自 Andrew Ng 的 Machine Learning 课程的第三周:<strong>多分类和过拟合技术</strong>的课堂记录，过拟合是机器学习优化的部分，主要讲解了以下几个内容:</p>
<ul>
<li>多类别的分类问题</li>
<li>过/欠拟合问题</li>
<li>解决过拟合的方法<ul>
<li>正则化方法</li>
</ul>
</li>
</ul>
<h1 id="多类别的分类问题"><a href="#多类别的分类问题" class="headerlink" title="多类别的分类问题"></a>多类别的分类问题</h1><p>在现实生活中多类别的问题更常见，比如：</p>
<ul>
<li>Email:邮件的自动分类，e.g. work,friends,family,hobby</li>
<li>医疗诊断：Notil,Cold,Flu</li>
<li>天气预测：Sunny,Cloudy,Rain,Snow</li>
</ul>
<p>在涉及到多分类问题，我们往往采取一种“One VS All”算法！在二分类问题上$Y = {0,1}$，在多分类上，我们将扩大$Y$的定义，即$Y =  {0,1,…,n}$。多分类的任务转化为$n + 1$个（<strong>+ 1因为索引从0开始</strong>）二分类问题；在每一个二分类任务，预测的“$Y$”是一个类概率。</p>
<p>\begin{align<em>}<br>&amp; y \in \lbrace0, 1 … n\rbrace \<br>&amp; h<em>\theta^{(0)}(x) = P(y = 0 | x ; \theta) \<br>&amp; h</em>\theta^{(1)}(x) = P(y = 1 | x ; \theta) \<br>&amp; \cdots \<br>&amp; h_\theta^{(n)}(x) = P(y = n | x ; \theta) \<br>&amp; \mathrm{prediction} = \max<em>i( h</em>\theta ^{(i)}(x) )<br>\end{align</em>}</p>
<p>我们基本上是选择其中一个类别的数据作为<strong>positive</strong>，然后将所有其他的类别数据为<strong>negative</strong>，这样将问题转化为一个二分类问题，反复这样做，对每一种情况应用<strong>二元logistic回归</strong>，然后将所有情况中预测的最高的那个类作为我们的预测。下图\ref{figure:onevsall} 显示了如何将3个类进行分类：</p>
<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/50713193.jpg" alt=""></p>
<ul>
<li>对于每一个类训练一个二元逻辑回归分类器来预测$y=1$的概率.</li>
<li>对于一个新的测试样本x，选择$h_{\theta}$最大的那一个类.</li>
</ul>
<h1 id="过-欠拟合问题"><a href="#过-欠拟合问题" class="headerlink" title="过/欠拟合问题"></a>过/欠拟合问题</h1><p>机器学习的目的不仅仅对训练集的拟合效果好，在测试集上我也要达到一样好！所以往往在机器学习建模时往往出现过拟合问题，就是对训练数据的预测达到100\%，但是在测试集上的效果很差！还有一种情况就是在训练集上的效果低，这叫欠拟合问题，如图所示</p>
<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/26710244.jpg" alt=""></p>
<p>考虑$x \in \mathbb{R}$预测$y$的问题。上图\ref{figure:overfit}的最左边的图显示了$y = \theta_{0} + \theta_1 x$拟合数据集$\mathrm{X}$的结果。我们看到数据不是真的在直线上，所以拟合程度不是很好。相反，如果我们添加了一个额外的特征$x^2$，用$y = \theta_0 + \theta_1x + \theta<em>2x^2$来拟合$y$,那么我们获得一个稍微更好的数据拟合（见中图）。看起来添加的特征越多越好，然而，增加太多特征也有一个危险：最右边的图是拟合5阶多项式$y = \sum</em>{j=0} ^5 \theta_j x^j$的结果，可以看到即使曲线完美地拟合了所有数据，但是也不会是个很好的模型。左边的图片是一个<strong>欠拟合</strong>的例子，而右侧的图就是过拟合的例子。</p>
<p><strong>欠拟合（高偏差，低方差）</strong>，它通常是由于假说函数过于简单或使用的特征太少而造成的。另一个极端，<strong>过拟合(低偏差，高方差)</strong>，它通常是由一个复杂的假说函数或者特征太太多引起的，而导致模型不能很好的泛化^[泛化指一个假设模型能够应用到新的样本数据的能力]。</p>
<p>解决过拟合问题两个主要策略：</p>
<ul>
<li>减少特征数量<ul>
<li>人工检查变量的数目，决定哪些变量重要，哪些变量不重要.</li>
<li>模型选择算法</li>
</ul>
</li>
<li>正则化<ul>
<li>当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响.</li>
<li>保留所有特性，但减少参数$\theta_j$的大小</li>
</ul>
</li>
</ul>
<h1 id="修改代价函数"><a href="#修改代价函数" class="headerlink" title="修改代价函数"></a>修改代价函数</h1><p>如果模型（假设/说函数）出现过拟合，我们可以减轻部分权重$\theta_j$，进而增加他们的代价。比如说，我们想让下面的函数看起来更加像二次方程</p>
<p>\begin{align<em>}<br>&amp;\theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4<br>\end{align</em>}</p>
<p>所以我们减弱$\theta_3x^3$和$\theta<em>4x^4$的影响.实际上, 如果不去掉这些特征或者改变假设函数$h</em>{\theta}(x)$的形式, 我们可以改变我们的代价函数：</p>
<p>\begin{align<em>}<br>&amp;min<em>\theta\ \dfrac{1}{2m}\sum</em>{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + 1000\cdot\theta_3^2 + 1000\cdot\theta_4^2<br>\end{align</em>}</p>
<p>在公式后面增加了两项用来增加$\theta_3$和$\theta_4$的代价，为了使得我们的代价函数接近0,所以不得不将$\theta_3$和$\theta_4$的值接近于0。在假说函数中，将大大降低$\theta_3x^3$和$\theta_4x^4$的值。因此,可以看到新的假设 (<strong>由粉红色曲线描述</strong>) 看起来更像一个二次函数,并且也更好的拟合我们训练数据。</p>
<p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/91568480.jpg" alt=""></p>
<p>我们还可以将所有的$\theta$参数求和来进行正则化处理,在代价函数最后增加一个<strong>正则项</strong> $\lambda \sum_{j=1}^n \theta<em>j^2$，其中$\lambda$是一个正则化参数（<strong>超参数</strong>）.如下所示.<br>\begin{align*}<br>&amp;min</em>\theta\ \dfrac{1}{2m}\  \sum<em>{i=1}^m (h</em>\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2<br>\end{align*}</p>
<h1 id="给线性回归模型增加正则项"><a href="#给线性回归模型增加正则项" class="headerlink" title="给线性回归模型增加正则项"></a>给线性回归模型增加正则项</h1><h2 id="Gradient-Descent-For-Fixed-Linear-Regression"><a href="#Gradient-Descent-For-Fixed-Linear-Regression" class="headerlink" title="Gradient Descent For Fixed Linear Regression"></a>Gradient Descent For Fixed Linear Regression</h2><p>\begin{align<em>}<br>&amp; \text{Repeat}\ \lbrace \<br>&amp; \ \ \ \ \theta_0 := \theta<em>0 - \alpha\ \frac{1}{m}\ \sum</em>{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \<br>&amp; \ \ \ \ \theta_j := \theta<em>j - \alpha\ \left[ \left( \frac{1}{m}\ \sum</em>{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &amp;\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2…n\rbrace\<br>&amp; \rbrace<br>\end{align</em>}</p>
<p>其中上面的$\theta_j$的更新公式也可以这样写：$$\theta_j := \theta<em>j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum</em>{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$.</p>
<h2 id="Normal-Equation"><a href="#Normal-Equation" class="headerlink" title="Normal Equation"></a>Normal Equation</h2><p>\begin{align<em>}<br>&amp; \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty, \; \text{where}\ \ L = \begin{bmatrix} 0 &amp; &amp; &amp; &amp; \<br> &amp; 1 &amp; &amp; &amp; \<br> &amp; &amp; 1 &amp; &amp; \<br>  &amp; &amp; &amp; \ddots &amp; \<br>   &amp; &amp; &amp; &amp; 1 \<br>\end{bmatrix}_{(n+1)×(n+1)}<br>\end{align</em>}</p>
<p><strong>前面提到过</strong>,如果 $m &lt; n$, 那么 $X^TX$ 是不可逆矩阵。然而, 当我们添加$\lambda L$后 $X^TX$变得可逆了。</p>
<h1 id="给逻辑回归模型增加正则项"><a href="#给逻辑回归模型增加正则项" class="headerlink" title="给逻辑回归模型增加正则项"></a>给逻辑回归模型增加正则项</h1><h2 id="Cost-Function"><a href="#Cost-Function" class="headerlink" title="Cost Function"></a>Cost Function</h2><p>\begin{align}<br>J(\theta) &amp;= - \frac{1}{m} \sum<em>{i=1}^m \large[ y^{(i)}\ \log (h</em>\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h<em>\theta(x^{(i)})) \large]\<br>\rightarrow J(\theta) &amp;= - \frac{1}{m} \sum</em>{i=1}^m \large[ y^{(i)}\ \log (h<em>\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h</em>\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2<br>\end{align}</p>
<p>公式(2)的第二个sum项$\sum_{j=1}^n \theta_j^2$中的$\theta_j$不包括$\theta_0$!</p>
<h2 id="Gradient-Descent-For-Fixed-Logistic-Regression"><a href="#Gradient-Descent-For-Fixed-Logistic-Regression" class="headerlink" title="Gradient Descent For Fixed Logistic Regression}"></a>Gradient Descent For Fixed Logistic Regression}</h2><p><img src="http://olrs8j04a.bkt.clouddn.com/17-12-21/82289725.jpg" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://olrs8j04a.bkt.clouddn.com/17-12-24/90775656.jpg"
               alt="Shen Hengheng" />
          <p class="site-author-name" itemprop="name">Shen Hengheng</p>
          <p class="site-description motion-element" itemprop="description">我是一名 AI 爱好者，着习于机器学习和深度学习。</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">34</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">34</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">34</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/rh01" target="_blank" title="github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/shenhengheng" target="_blank" title="twitter">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/shenhengheng" target="_blank" title="weibo">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.readailib.com" title="readailib" target="_blank">readailib</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://rh01.github.io" title="ShenHengheng" target="_blank">ShenHengheng</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://data.shenhengheng.xyz" title="Projects" target="_blank">Projects</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://iotppt.shenhengheng.xyz" title="IoT演讲" target="_blank">IoT演讲</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<div class="copyright" >

  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shen Hengheng</span>
</div>




<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

本站总访问量 <span id="busuanzi_value_site_pv"></span> &nbsp&nbsp&nbsp
您是第<span id="busuanzi_value_site_uv"></span>个来到的小伙伴


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"shenhengheng"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  

  

  

  


  

<!-- 背景动画 -->
<script type="text/javascript" src="/js/src/particle.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


</body>

</html>
